{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e0a074-8e6c-4ac9-be16-29b722423a8b",
   "metadata": {},
   "source": [
    "# Contimuous Control Project Report\n",
    "##Â The Algorithm \n",
    "\n",
    "The implementation used to solve this project was a Deep Deterministic Policy Grdient (DDPG) agnet [1](https://arxiv.org/abs/1509.02971). This agent is comprised of four seperate Deep Neural Networks (DNN), a local and target `actor_nn` alongside a local and target `critic_nn`. The actor makes predictions for the agents next actions and the critic then evalueates them. Both th eactor and critic learn at the same time through bacth experience replay, and the target networks are `soft_updated` after each set of 20 time steps (i.e. once each of the 20 arms in the environment has chosen an action). \n",
    "\n",
    "## Results \n",
    "\n",
    "I had significant issues getting this agent to learn, and it seemed like most attempt were learning, but very slowly and not quite hitting the mark. I did eventually come to a setup that seemed to work reasonably well (although it did take 300 episodedes to train). Below are the parameters that eventually solved the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427e6805-fbcf-4173-96f1-6649fa7347ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "LR_A = 1e-4\n",
    "LR_C = 1e-3\n",
    "W_DECAY = 0\n",
    "REPLAY_EVERY = 20\n",
    "\n",
    "action_layers = [256, 256, 128]\n",
    "critic_layers = [256, 128, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15de8fa-c850-4c0c-ad77-6f608f7ccb76",
   "metadata": {},
   "source": [
    "The main contributor to the performace by far seemed to be the NN structures (`action_layers` & `critic_layers`). Below is the score graph of the final trained agent (average of all 20 agents at each epoch) along with a gif of it in action. To see this live, feel free to look at `testing.ipynb` and run the final section of cells to reload and run the trained model. \n",
    "\n",
    "<img src=\"ouput_rewards.png\">\n",
    "\n",
    "<img src=\"trained_arms.gif\">\n",
    "\n",
    "## Further work \n",
    "\n",
    "There is definately much to be done to optimise this agent further. I am sure that with the correct NN structures, this agent can solve the environment well within 200 episodes, however, I have struggled to find this. What will likely yeild results is exploration into different combinations of deep and/or wide NNs for the actor and critic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9225bf-3177-48a4-ac0c-fa7dc647e79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
